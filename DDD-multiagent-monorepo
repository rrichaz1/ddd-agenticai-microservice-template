1. Keep Core Domain Objects for Key Concepts Only
Define lightweight domain models for:

Encounter

ClinicalNote (with note_type, text, etc.)

Possibly LabResult, VitalSign (if they become central to agent workflows)

These can be implemented as @dataclasses or Pydantic models in domain/:

python
Copy
Edit
# domain/clinical_note/models.py
from pydantic import BaseModel

class ClinicalNote(BaseModel):
    note_type: str
    text: str
2. Foster Domain Isolation Without Full Marshalling
Use adapters/fhir_parser.py to parse FHIR/JSON and return partial domain models or dictionaries.

Only convert to full ClinicalNote, etc., in places where behavior or validation is required.

3. Retain a Strong application/ Layer
Application services orchestrate workflows (e.g., calling agents, preprocessing, postprocessing).

Application is where most expansion and orchestration logic lives.

4. Define Bounded Contexts as Needed
You can eventually organize domain/clinical, domain/lab, domain/vital, etc., if complexity grows.

Each bounded context should own its agents, services, and models.

5. Evaluation and Prompt Versions as First-Class Concepts
Treat PromptVersion and EvaluationRun as domain objects or value objects in an evaluation/ context.

This aligns with your CI/CD goals and allows reproducibility.



repo-root/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”‚   â”œâ”€â”€ clinical_note/
â”‚   â”‚   â”‚   â”œâ”€â”€ encounter/
â”‚   â”‚   â”‚   â””â”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ application/
â”‚   â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ dtos/
â”‚   â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”‚   â”œâ”€â”€ fhir_parser.py
â”‚   â”‚   â”‚   â”œâ”€â”€ azure_openai.py
â”‚   â”‚   â”‚   â”œâ”€â”€ keyvault_client.py
â”‚   â”‚   â”‚   â””â”€â”€ blob_writer.py
â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_soap_agent.py
â”‚   â”‚   â”‚   â””â”€â”€ summarization_agent.py
â”‚   â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”‚   â”œâ”€â”€ eval_suite.py         # Evaluation framework for CI/CD
â”‚   â”‚   â”‚   â””â”€â”€ fixtures/             # Gold standards, test inputs
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ main.py               # FastAPI entrypoint
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ test_agents/
â”‚   â”‚   â”œâ”€â”€ test_api/
â”‚   â”‚   â””â”€â”€ test_eval/
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ experimentation/
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ version_001_extract.txt
â”‚   â”‚   â””â”€â”€ version_002_summary.txt
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â””â”€â”€ prompt_dev.ipynb          # For trying new prompt versions
â”‚   â”œâ”€â”€ runners/
â”‚   â”‚   â””â”€â”€ local_eval_runner.py      # Compare outputs vs gold std
â”‚   â””â”€â”€ README.md                     # How to test prompts, log metrics
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â””â”€â”€ index.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â””â”€â”€ OutputViewer.tsx
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â””â”€â”€ package.json                  # Next.js or Vite-based frontend
â”‚
â”œâ”€â”€ streamlit_app/                # ðŸ§ª DS-friendly local UI
â”‚   â”œâ”€â”€ main.py                   # Streamlit entry point
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 1_Prompt_Tester.py    # Upload JSON, try prompt
â”‚   â”‚   â””â”€â”€ 2_Prompt_Comparer.py  # Side-by-side version eval
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ load_prompt.py
â”‚   â”‚   â””â”€â”€ run_agent.py          # Can call FastAPI or agents directly
â”‚   â””â”€â”€ requirements.txt          # Only Streamlit dependencies
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                    # Run eval_suite.py before deploy
â”œâ”€â”€ README.md
â””â”€â”€ pyproject.toml or setup.py

 Example Workflow for Data Scientist
Clone repo, go to streamlit_app/

Run: streamlit run main.py

Choose a prompt version, paste or upload clinical JSON

View generated output, copy it to Gold Standard if good

Save prompt version (e.g., as v003_extract.txt)

Ping engineers to run eval_suite.py with new prompt

ðŸ”„ Streamlit Use Modes
Mode	Streamlit App Calls	Pros	Notes
Local/Offline	Runs run_agent.py directly	Super fast, no server dependency	Great for iteration, but may bypass actual infra
Connected	Calls backend.api.main endpoints	Realistic view of deployed behavior	Ensure backend is running

You can toggle between modes with a simple config["MODE"] = "local" vs "remote".